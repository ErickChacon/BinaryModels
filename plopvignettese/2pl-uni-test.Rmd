---
title: "Two Parameter Logistic Model"
author: "Erick A. Chacon-Montalvan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Two Parameter Logistic Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::opts_chunk$set(fig.width = 6, fig.height = 4)
knitr::opts_chunk$set(comment = "#>")
options(width = 100)
```


In this vignette, we show how to use the package to fit a 2 parameter model.

## Required packages

```{r}
rm(list = ls())
library(spmirt)
library(datasim)
library(coda)
library(tidyverse)
```

# Example 1

## Simulation of the data

```{r}
q <- 10
n <- 300
difficulty <- matrix((1:q - 5)/10 * 2, nrow = 1)
discrimination <- matrix(seq(0.4, 1.5, length.out = q), nrow = 1)
discrimination[1,1] = 2

f <- list(
  prob ~ mfa(ones, beta = get("difficulty")) +
    re(id, sigma = 1, q = get("q")):mfa(ones, beta = get("discrimination")),
  size ~ I(1)
  )

data_long <- sim_model(formula = f,
                        link_inv = list(pnorm, identity),
                        generator = rbinom,
                        responses = q,
                        n = n
                        # ,
                        # seed = 3
                        )

data_long <- dplyr::rename(data_long, subject = id, ability = re.id.prob,
                           item = response_label, y = response)
```

```{r, echo = FALSE, results = 'asis'}
knitr::kable(head(data_long, 20))
```

```{r}
explor <- data_long %>%
  group_by(subject) %>%
  summarize(endorse = mean(y), ability = unique(ability))
ggplot(explor, aes(ability, endorse)) + geom_point()

```


## Fitting the model

```{r}
iter <- 1000
system.time(samples <- ifa_gibbs_test(data_long$y, n, q, iter))
purrr::map(samples, dim)
samples$firstA
samples$firstA <- NULL
plot(samples$a[, 1:2], pch = 19, col = rgb(0, 0, 0, 0.2))
points(-discrimination[1], -discrimination[2], pch = 19, col = 2)
points(discrimination[1], discrimination[2], pch = 19, col = 2)
```

## Evaluate the results

```{r}

# Organize and summarise output
samples_after_burin <- samples %>% purrr::map(~ .[(0.5 * iter):iter, ])
samples_params <- samples_after_burin[1:3]
real_params <- list(theta = unique(data_long$ability),
                    c = as.numeric(difficulty),
                    a = as.numeric(discrimination))
samples_params_summary <- samples_params %>%
  map(~ apply(., 2, function (x) quantile(x, c(0.025, 0.5, 0.975)))) %>%
  map(~ as_tibble(t(.))) %>%
  map(~ setNames(., make.names(names(.)))) %>%
  map2(real_params, ~ mutate(.x, param = .y)) %>%
  setNames(c("Latent ability", "Item difficulty", "Item discrimination"))

# Visualize convergence
opar <- par(mfrow = c(2, 2), mar = c(2, 1.9, 2, 0.6))
# layout(matrix(1:2))
map(samples_after_burin, ~ plot(mcmc(.[, 1:5]), auto.layout = FALSE))
par(opar)

# Visualize results
samples_params_summary %>%
  map(~ ggplot(., aes(X50., param)) +
          geom_errorbar(aes(ymin = X2.5., ymax = X97.5.)) +
          geom_point(col = 2)
        )
```


# Example 2

## Simulation of the data

```{r}
q <- 10
n <- 300
difficulty <- matrix((1:q - 5)/10 * 2, nrow = 1)
discrimination <- matrix(seq(0.4, 1.5, length.out = q), nrow = 1)
discrimination[1,1] = 2

f <- list(
  prob ~ mfa(ones, beta = get("difficulty")) +
    re(id, sigma = 1, q = get("q")):mfa(ones, beta = get("discrimination")),
  size ~ I(1)
  )

data_long <- sim_model(formula = f,
                        link_inv = list(pnorm, identity),
                        generator = rbinom,
                        responses = q,
                        n = n
                        # ,
                        # seed = 1
                        )

data_long <- dplyr::rename(data_long, subject = id, ability = re.id.prob,
                           item = response_label, y = response)
```

```{r, echo = FALSE, results = 'asis'}
knitr::kable(head(data_long, 20))
```

```{r}
explor <- data_long %>%
  group_by(subject) %>%
  summarize(endorse = mean(y), ability = unique(ability))
ggplot(explor, aes(ability, endorse)) + geom_point()

```


## Fitting the model

```{r}
# set.seed(4)
# set.seed(1)
iter <- 1000
system.time(samples <- ifa_gibbs_test(data_long$y, n, q, iter))
purrr::map(samples, dim)
samples$firstA
samples$firstA <- NULL
plot(samples$a[, 1:2], pch = 19, col = rgb(0, 0, 0, 0.2))
points(-discrimination[1], -discrimination[2], pch = 19, col = 2)
points(discrimination[1], discrimination[2], pch = 19, col = 2)
```

## Evaluate the results

```{r}

# Organize and summarise output
samples_after_burin <- samples %>% purrr::map(~ .[(0.5 * iter):iter, ])
samples_params <- samples_after_burin[1:3]
real_params <- list(theta = unique(data_long$ability),
                    c = as.numeric(difficulty),
                    a = as.numeric(discrimination))
samples_params_summary <- samples_params %>%
  map(~ apply(., 2, function (x) quantile(x, c(0.025, 0.5, 0.975)))) %>%
  map(~ as_tibble(t(.))) %>%
  map(~ setNames(., make.names(names(.)))) %>%
  map2(real_params, ~ mutate(.x, param = .y)) %>%
  setNames(c("Latent ability", "Item difficulty", "Item discrimination"))

# Visualize convergence
opar <- par(mfrow = c(2, 2), mar = c(2, 1.9, 2, 0.6))
# layout(matrix(1:2))
map(samples, ~ plot(mcmc(.[, 1:5]), auto.layout = FALSE))
par(opar)

# Visualize results
samples_params_summary %>%
  map(~ ggplot(., aes(X50., param)) +
          geom_errorbar(aes(ymin = X2.5., ymax = X97.5.), width = 0.1) +
          geom_point(col = 2)
        )



# c_sam <- mcmc(-samples2$c)
# plot(c_sam)
#
# plot(discrimination, means$a)
# abline(0, 1)
# abline(0, -1)
#
# a_sam <- mcmc(samples2$a)
# plot(a_sam)
#
# plot(plop$ability, medians$theta)
# abline(0, 1)
# abline(0, -1)
```

